{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute uniqueness\n",
    "\n",
    "Already included this in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.yolo as fouy\n",
    "\n",
    "# for computing embeddings\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "import pickle\n",
    "\n",
    "# for filtering datasets to create views + tags\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "classes = ['Pasiphaea', 'Poeobius meseres', 'Siphonophorae', 'Ctenophora', 'Medusae', 'Eusergestes similis', 'Octopus', 'Larvacean', 'Fish', 'Squid', 'Mysida', 'Worm', 'Echinoderm', 'Other', 'Crustacea', 'Anemone', 'Equipment', 'Coral', 'Sponge', 'Pennatulacea', 'Euphausiacea']\n",
    "\n",
    "existent_datasets = fo.list_datasets()\n",
    "print(existent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ds in existent_datasets:\n",
    "#     dataset = fo.load_dataset(ds)\n",
    "#     dataset.delete()\n",
    "\n",
    "# existent_datasets = fo.list_datasets()\n",
    "# print(existent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2 = fo.load_dataset(\"2023.04.07.17.29.18\")\n",
    "# dataset2.delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"2023.04.07.19.54.02\"\n",
    "dataset_dir=\"/mnt/c/Users/sabri/Documents/github/thesis/datasets/raw\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from image directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fo.Dataset.from_images_dir(dataset_dir)\n",
    "\n",
    "# print(f\"Dataset created: {dataset_name}.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ground_truth for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add labels\n",
    "# import fiftyone.utils.yolo as fouy\n",
    "\n",
    "# fouy.add_yolo_labels(\n",
    "#     sample_collection=dataset, \n",
    "#     label_field=\"ground_truth\",\n",
    "#     labels_path=\"/mnt/c/Users/sabri/Documents/github/thesis/datasets/raw/labels\",\n",
    "#     classes=classes,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.pprint(dataset.stats(include_media=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.delete_sample_field('predictions_tflite16') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import fiftyone as fo\n",
    "\n",
    "def tag_samples(dataset_name, processed_dir):\n",
    "    \"\"\"\n",
    "    This function tags samples in a given FiftyOne dataset with the tag names extracted from the file names of the provided directory.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the FiftyOne dataset to be updated with tags.\n",
    "    processed_dir (str): The path to the directory containing files with the image file paths. The file names without the extension will be used as tag names.\n",
    "\n",
    "    Functionality:\n",
    "    1. Loads the specified dataset using FiftyOne.\n",
    "    2. Iterates through all the files in the given directory, recursively.\n",
    "    3. Extracts the tag name from the file name without extension.\n",
    "    4. Opens the file and iterates through the image file paths.\n",
    "    5. Matches the image path in the dataset and retrieves the sample.\n",
    "    6. Appends the tag name to the sample's tag list, if not already present, and saves the sample.\n",
    "    7. In case of any ValueError, prints an error message with the affected image path.\n",
    "    \"\"\"\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    for file_path in glob.glob(processed_dir, recursive=True):\n",
    "        print(f\"Getting images in {file_path}\")\n",
    "        tag_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in tqdm(file):\n",
    "                image_path = line.strip()\n",
    "                try:\n",
    "                    sample = dataset.match({\"filepath\": image_path}).first()\n",
    "                    if tag_name not in sample.tags:\n",
    "                        sample.tags.append(tag_name)\n",
    "                        sample.save()\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: {e} at {image_path} in {file_path}\")\n",
    "                    break\n",
    "\n",
    "dataset_name = \"2023.04.07.19.54.02\"\n",
    "processed_dir = '/mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/**/*.txt'\n",
    "\n",
    "# already done, so comment out\n",
    "# tag_samples(dataset_name, processed_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add predictions\n",
    "\n",
    "More info here: https://docs.voxel51.com/user_guide/dataset_creation/index.html#model-predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUV predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fouy.add_yolo_labels(\n",
    "#     sample_collection=dataset, \n",
    "#     label_field=\"predictions_onnx\",\n",
    "#     labels_path=\"/mnt/c/Users/sabri/Documents/github/thesis/artifacts/predictions/predict_onnx/labels\",\n",
    "#     classes=classes,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fouy.add_yolo_labels(\n",
    "#     sample_collection=dataset, \n",
    "#     label_field=\"predictions_tflite16\",\n",
    "#     labels_path=\"/mnt/c/Users/sabri/Documents/github/thesis/artifacts/predictions/predict_tflite16/labels\",\n",
    "#     classes=classes,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App instance\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view1 = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"Eusergestes similis\") # not what i want. excludes other labels, I want to keep them like in 'shrimp'\n",
    "print(len(view1))\n",
    "# dataset.save_view(\"shrimp_2\", view1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.list_saved_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_saved_view_info('shrimp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "How to do this explained here: https://docs.voxel51.com/tutorials/image_embeddings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open('embeddings.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings\n",
    "I have already done this and pickled it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fiftyone.zoo as foz\n",
    "\n",
    "# # Compute embeddings\n",
    "# # You will likely want to run this on a machine with GPU, as this requires\n",
    "# # running inference on 10,000 images\n",
    "# model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "# embeddings = dataset.compute_embeddings(model)\n",
    "\n",
    "# # Open a file and use dump()\n",
    "# with open('embeddings.pkl', 'wb') as file:\n",
    "#     # A new file will be created\n",
    "#     pickle.dump(embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute visualization\n",
    "results = fob.compute_visualization(\n",
    "    dataset, embeddings=embeddings, seed=42, brain_key=\"img_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object patch embeddings\n",
    "fob.compute_visualization(\n",
    "    dataset, patches_field=\"ground_truth\", brain_key=\"gt_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(results))\n",
    "print(results.points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(dataset, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_similarity(dataset, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing patch embeddings is breaking the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# dataset.compute_patch_embeddings(\n",
    "#     model, \n",
    "#     \"ground_truth\", \n",
    "#     embeddings_field = \"gt_embed\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.list_brain_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.load_brain_results(\"img_viz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings colored by ground truth label\n",
    "plot = results.visualize(labels=\"ground_truth.detections.label\")\n",
    "plot.show(height=520)\n",
    "\n",
    "# # Attach plot to session\n",
    "# session.plots.attach(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App instance\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "auv_view = dataset.match(F(\"filepath\").contains_str(\"output\"))\n",
    "auv_view.tag_samples(\"auv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rov_view = dataset.match(~F(\"filepath\").contains_str(\"output\"))\n",
    "rov_view.tag_samples(\"rov\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggragation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of samples in the dataset\n",
    "count = dataset.count()\n",
    "print(count)\n",
    "# 200\n",
    "\n",
    "# Compute the number of samples with `predictions`\n",
    "count = dataset.count(\"predictions_auv450\")\n",
    "print(count)\n",
    "# 200\n",
    "\n",
    "# Compute the number of detections in the `ground_truth` field\n",
    "count = dataset.count(\"ground_truth.detections\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"predictions_auv450.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.bounds(\"uniqueness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_sample_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25 = dataset.match_tags(\"train25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25.list_aggregations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25.mean(\"uniqueness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
