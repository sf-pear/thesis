{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute uniqueness\n",
    "\n",
    "Already included this in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023.04.07.19.54.02', 'auv', 'merged', 'rov']\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "existent_datasets = fo.list_datasets()\n",
    "print(existent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ds in existent_datasets:\n",
    "#     dataset = fo.load_dataset(ds)\n",
    "#     dataset.delete()\n",
    "\n",
    "# existent_datasets = fo.list_datasets()\n",
    "# print(existent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2 = fo.load_dataset(\"2023.04.07.17.29.18\")\n",
    "# dataset2.delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add predictions\n",
    "\n",
    "More info here: https://docs.voxel51.com/user_guide/dataset_creation/index.html#model-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Pasiphaea', 'Poeobius meseres', 'Siphonophorae', 'Ctenophora', 'Medusae', 'Eusergestes similis', 'Octopus', 'Larvacean', 'Fish', 'Squid', 'Mysida', 'Worm', 'Echinoderm', 'Other', 'Crustacea', 'Anemone', 'Equipment', 'Coral', 'Sponge', 'Pennatulacea', 'Euphausiacea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.yolo as fouy\n",
    "\n",
    "# And add model predictions\n",
    "fouy.add_yolo_labels(dataset,\n",
    "    \"predictions\",\n",
    "    \"/tmp/yolov4/predictions\",\n",
    "    classes,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List saved views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.list_saved_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_saved_view_info('auv_val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise embeddings\n",
    "Try this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023.04.07.19.54.02', 'auv', 'merged', 'rov']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "import pickle\n",
    "\n",
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"2023.04.07.19.54.02\"\n",
    "dataset_dir=\"/mnt/c/Users/sabri/Documents/github/thesis/datasets/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fo.Dataset.from_images_dir(dataset_dir)\n",
    "\n",
    "# print(f\"Dataset created: {dataset_name}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ['Pasiphaea',\n",
    "# 'Poeobius meseres',\n",
    "# 'Siphonophorae',\n",
    "# 'Ctenophora',\n",
    "# 'Medusae',\n",
    "# 'Eusergestes similis',\n",
    "# 'Octopus',\n",
    "# 'Larvacean',\n",
    "# 'Fish',\n",
    "# 'Squid',\n",
    "# 'Mysida',\n",
    "# 'Worm',\n",
    "# 'Echinoderm',\n",
    "# 'Other',\n",
    "# 'Crustacea',\n",
    "# 'Anemone',\n",
    "# 'Equipment',\n",
    "# 'Coral',\n",
    "# 'Sponge',\n",
    "# 'Pennatulacea', \n",
    "# 'Euphausiacea']\n",
    "\n",
    "# # add labels\n",
    "# import fiftyone.utils.yolo as fouy\n",
    "\n",
    "# fouy.add_yolo_labels(\n",
    "#     sample_collection=dataset, \n",
    "#     label_field=\"ground_truth\",\n",
    "#     labels_path=\"/mnt/c/Users/sabri/Documents/github/thesis/datasets/raw/labels\",\n",
    "#     classes=classes,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sample: {\n",
      "    'id': '643058bca3e3e8563050496d',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/mnt/c/Users/sabri/Documents/github/thesis/datasets/raw/images/0001b461-9780-4bd1-8656-425891402580.png',\n",
      "    'tags': ['rov'],\n",
      "    'metadata': <ImageMetadata: {\n",
      "        'size_bytes': 389887,\n",
      "        'mime_type': 'image/png',\n",
      "        'width': 720,\n",
      "        'height': 486,\n",
      "        'num_channels': 3,\n",
      "    }>,\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '643058c4a3e3e8563050a836',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'Pasiphaea',\n",
      "                'bounding_box': [0.461805, 0.44547300000000006, 0.118056, 0.13786],\n",
      "                'mask': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "    'uniqueness': 0.17149335231831642,\n",
      "    'predictions_auv450': None,\n",
      "    'predictions_onnx': None,\n",
      "    'predictions_tflite16': None,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.first())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv/auv_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [00:01, 247.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv/auv_val.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 280.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train100.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 286.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train125.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:00, 275.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train150.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:00, 259.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train175.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [00:00, 272.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train200.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 273.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train225.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:00, 263.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train25.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 228.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train250.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:00, 259.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train275.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [00:01, 264.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train300.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:01, 267.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train325.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325it [00:01, 272.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train350.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [00:01, 269.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train375.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "375it [00:01, 268.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train400.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:01, 275.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train425.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "425it [00:01, 270.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train450.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [00:01, 275.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train50.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 260.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/auv_increasing_random/train75.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 270.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/both/both_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14691it [00:20, 726.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/rov/rov_test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4748it [00:06, 776.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/rov/rov_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14241it [00:17, 793.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/rov/rov_val.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4747it [00:06, 773.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/unique_least/least_unique.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 167.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images in /mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/unique_most/most_unique.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 147.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import fiftyone as fo\n",
    "\n",
    "def tag_samples(dataset_name, processed_dir):\n",
    "    \"\"\"\n",
    "    This function tags samples in a given FiftyOne dataset with the tag names extracted from the file names of the provided directory.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the FiftyOne dataset to be updated with tags.\n",
    "    processed_dir (str): The path to the directory containing files with the image file paths. The file names without the extension will be used as tag names.\n",
    "\n",
    "    Functionality:\n",
    "    1. Loads the specified dataset using FiftyOne.\n",
    "    2. Iterates through all the files in the given directory, recursively.\n",
    "    3. Extracts the tag name from the file name without extension.\n",
    "    4. Opens the file and iterates through the image file paths.\n",
    "    5. Matches the image path in the dataset and retrieves the sample.\n",
    "    6. Appends the tag name to the sample's tag list, if not already present, and saves the sample.\n",
    "    7. In case of any ValueError, prints an error message with the affected image path.\n",
    "    \"\"\"\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    for file_path in glob.glob(processed_dir, recursive=True):\n",
    "        print(f\"Getting images in {file_path}\")\n",
    "        tag_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in tqdm(file):\n",
    "                image_path = line.strip()\n",
    "                try:\n",
    "                    sample = dataset.match({\"filepath\": image_path}).first()\n",
    "                    if tag_name not in sample.tags:\n",
    "                        sample.tags.append(tag_name)\n",
    "                        sample.save()\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: {e} at {image_path}\")\n",
    "\n",
    "dataset_name = \"2023.04.07.19.54.02\"\n",
    "processed_dir = '/mnt/c/Users/sabri/Documents/github/thesis/datasets/processed/**/*.txt'\n",
    "\n",
    "tag_samples(dataset_name, processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    'samples_count': 24265,\n",
      "    'samples_bytes': 17069973,\n",
      "    'samples_size': '16.3MB',\n",
      "    'media_bytes': 40973704444,\n",
      "    'media_size': '38.2GB',\n",
      "    'total_bytes': 40990774417,\n",
      "    'total_size': '38.2GB',\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fo.pprint(dataset.stats(include_media=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Pasiphaea',\n",
    "'Poeobius meseres',\n",
    "'Siphonophorae',\n",
    "'Ctenophora',\n",
    "'Medusae',\n",
    "'Eusergestes similis',\n",
    "'Octopus',\n",
    "'Larvacean',\n",
    "'Fish',\n",
    "'Squid',\n",
    "'Mysida',\n",
    "'Worm',\n",
    "'Echinoderm',\n",
    "'Other',\n",
    "'Crustacea',\n",
    "'Anemone',\n",
    "'Equipment',\n",
    "'Coral',\n",
    "'Sponge',\n",
    "'Pennatulacea', \n",
    "'Euphausiacea']\n",
    "\n",
    "# add labels\n",
    "import fiftyone.utils.yolo as fouy\n",
    "\n",
    "fouy.add_yolo_labels(\n",
    "    sample_collection=dataset, \n",
    "    label_field=\"predictions_tflite16\",\n",
    "    labels_path=\"/mnt/c/Users/sabri/Documents/github/thesis/artifacts/predictions/predict_tflite16/labels\",\n",
    "    classes=classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App instance\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "How to do this explained here: https://docs.voxel51.com/tutorials/image_embeddings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open('embeddings.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute embeddings\n",
    "I have already done this and pickled it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fiftyone.zoo as foz\n",
    "\n",
    "# # Compute embeddings\n",
    "# # You will likely want to run this on a machine with GPU, as this requires\n",
    "# # running inference on 10,000 images\n",
    "# model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "# embeddings = dataset.compute_embeddings(model)\n",
    "\n",
    "# # Open a file and use dump()\n",
    "# with open('embeddings.pkl', 'wb') as file:\n",
    "#     # A new file will be created\n",
    "#     pickle.dump(embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute visualization\n",
    "results = fob.compute_visualization(\n",
    "    dataset, embeddings=embeddings, seed=42, brain_key=\"img_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object patch embeddings\n",
    "fob.compute_visualization(\n",
    "    dataset, patches_field=\"ground_truth\", brain_key=\"gt_viz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(results))\n",
    "print(results.points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(dataset, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_similarity(dataset, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing patch embeddings is breaking the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# dataset.compute_patch_embeddings(\n",
    "#     model, \n",
    "#     \"ground_truth\", \n",
    "#     embeddings_field = \"gt_embed\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.list_brain_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.load_brain_results(\"img_viz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings colored by ground truth label\n",
    "plot = results.visualize(labels=\"ground_truth.detections.label\")\n",
    "plot.show(height=520)\n",
    "\n",
    "# # Attach plot to session\n",
    "# session.plots.attach(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App instance\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "auv_view = dataset.match(F(\"filepath\").contains_str(\"output\"))\n",
    "auv_view.tag_samples(\"auv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
